{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import tensorflow as tf\n",
    "data = load_iris()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# label one-hot 编码\n",
    "ohe = OneHotEncoder()\n",
    "y_bin = []\n",
    "print(np.unique(data.target))\n",
    "for i in np.unique(data.target):\n",
    "    y_bin.append([i])\n",
    "ohe.fit(y_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def batch_iter(data, label, batch_size, num_epoch):\n",
    "    data_size = len(data)\n",
    "    num_batch_per_epoch = int(data_size / batch_size) + 1\n",
    "    if data_size % batch_size == 0:\n",
    "        num_batch_per_epoch = int(data_size / batch_size)\n",
    "    for epoch in range(num_epoch):\n",
    "        # with shuffle\n",
    "        shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "        \n",
    "        shuffle_target = label[shuffle_indices]\n",
    "        shuffle_data = np.array(data[shuffle_indices])\n",
    "        \n",
    "        # batch generation\n",
    "        for batch_num in range(num_batch_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            target = []\n",
    "            for a in shuffle_target[start_index: end_index]:\n",
    "                target.append([a])\n",
    "            yield shuffle_data[start_index: end_index, :], ohe.transform(target).toarray(), epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(object):\n",
    "    def __init__(self, feature_num, num_classes):\n",
    "        self.input_x = tf.placeholder(tf.float32, [None, feature_num])\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "        \n",
    "        self.W = tf.Variable(tf.zeros(shape=[feature_num, num_classes]))\n",
    "        self.b = tf.Variable(tf.zeros([num_classes]))\n",
    "        \n",
    "        # output, linear model\n",
    "        y = tf.nn.softmax(tf.nn.xw_plus_b(self.input_x, self.W, self.b))\n",
    "        \n",
    "        # loss definition\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # softmax\n",
    "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.input_y, logits=y)\n",
    "            self.loss = tf.reduce_mean(entropy)\n",
    "            correct_prediction = tf.equal(tf.arg_max(y, 1), tf.arg_max(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.86310369  -1.11319355  -3.74991014]\n",
      " [  6.93409701  -0.56457404  -6.36952297]\n",
      " [-12.25764049   1.57709252  10.68054797]\n",
      " [ -6.26149609  -4.96174117  11.22323726]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "import datetime\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "train_X, test_X, train_y, test_y = train_test_split(data.data, data.target, test_size=0.2, random_state=0)\n",
    "\n",
    "# sklean logistic regression\n",
    "reg = LogisticRegression(C=999999, solver=\"newton-cg\", multi_class=\"multinomial\")\n",
    "reg.fit(train_X, train_y)\n",
    "\n",
    "print(reg.coef_.T)\n",
    "\n",
    "test_target = []\n",
    "for i in test_y:\n",
    "    test_target.append([i])\n",
    "test_target = ohe.transform(test_target).toarray()\n",
    "\n",
    "batch_size = len(train_y)\n",
    "epoch_num = 1000\n",
    "\n",
    "# new saver\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    mlp = MLP(feature_num=4, num_classes=3)\n",
    "    train_step = tf.train.GradientDescentOptimizer(0.05).minimize(mlp.loss)\n",
    "    \n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for batch_xs, batch_ys, epoch in batch_iter(train_X, train_y, batch_size, epoch_num):\n",
    "        # batch data\n",
    "        feed_dict = {\n",
    "            mlp.input_x: batch_xs,\n",
    "            mlp.input_y: batch_ys\n",
    "        }\n",
    "        # given batch data, get result\n",
    "        _, loss, accuracy = sess.run([train_step, mlp.loss, mlp.accuracy], feed_dict)\n",
    "        if epoch % 50 == 0:\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            test_accuracy = sess.run(mlp.accuracy, feed_dict={mlp.input_x:test_X, mlp.input_y:test_target})\n",
    "            print(\"{} epoch: {:g}, loss: {:g}, accuracy: {:g}, test_accuracy: {:g}\".format(time_str, epoch, loss, accuracy, test_accuracy))\n",
    "    \n",
    "    print(mlp.W.eval())\n",
    "    \n",
    "    # save model\n",
    "#     save_path = saver.save(sess, \"./model/model.ckpt\")\n",
    "#     print(\"model saved in file: \", save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论\n",
    "0. 使用iris数据（共150条数据），80%的数据作为训练数据（120条），20%的数据作为测试数据（30条）\n",
    "1. 学习率设置为0.05，大约在650次迭代后，准确率可以达到100%\n",
    "2. 权重矩阵如下：\n",
    "```\n",
    "[[-1.1738335   0.50324905  0.6705846 ]\n",
    " [-1.279821   -0.2514371   1.5312573 ]\n",
    " [ 2.187848   -0.0102499  -2.1775951 ]\n",
    " [ 1.7318009  -0.71315885 -1.0186411 ]]\n",
    "```\n",
    "2. 从weight权重矩阵看：\n",
    "    * 第一类与后两个特征关系较大\n",
    "    * 第二类与第一个特征关系较大\n",
    "    * 第三类与前两个特征关系较大\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
